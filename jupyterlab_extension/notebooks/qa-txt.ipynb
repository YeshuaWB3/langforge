{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67820df-cf47-4f39-8745-9656448564af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create a Question Answering Chat\n",
    "\n",
    "This is a quick template for creating a question answering chat with ChatGPT and ðŸ¦œðŸ”— LangChain.\n",
    "\n",
    "We load an example document and create an index using OpenAI text embeddings. Then, we can chat about the contents of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67048376-0aa6-4eab-a130-7cca75e556a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%setup langchain openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb6ab3-ce1f-4828-bb94-c12be9a31366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import urllib.request\n",
    "\n",
    "# retrieve the state of the union speech\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\", \"state_of_the_union.txt\")\n",
    "\n",
    "txts = [\n",
    "    \"state_of_the_union.txt\"\n",
    "];\n",
    "\n",
    "documents = []\n",
    "\n",
    "for txt in txts:\n",
    "    loader = TextLoader(txt)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e799f0-ff67-44d7-a141-7d378ae19179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "# if you want GPT-4: \n",
    "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory, get_chat_history=lambda inputs: inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
